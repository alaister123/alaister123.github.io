---
title: "Investigate & Mitigate Sex Bias in Occupation for Text-to-Image Models"
excerpt: "Why the prompt \"Doctor\" generates male images while \"Nurse\" generate female images?<br/><img src='/images/289g.png'>"
collection: portfolio
---

Investigated why typing the word "doctor" in text to image models will mainly generate male doctors. Based on the investigated results, tried to mitigate the biases using different techniques including embedding edits, prompt engineering, as well as fine-tuning the model. Embedding edits worked the best but it is still not 50/50. There may be much deeper reason into why this phenomenon appears. 